---
title: "Data Pipeline and Cleaning"
author: "Christoper Chan"
date: "`r format(Sys.time(), '%H:%M %d %B %Y')`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction

This notebook is a part 0 of the Devereux Slough time series project. We wrote 
functions that aggregate and cleaning the data as well as some light feature
engineering.

We've chosen to use the library here because it's simplicity and consistency.
Because we have seperate directories for notebooks and data we needed a way to 
navigate between them. While base R does offer this functionality, pathing is 
relative to the current working directory which changes based on where I created
a document. here() offers absolute pathing by setting up a root directory.
```{r libraries, warning=FALSE, message=FALSE}
library(here)
library(tidyverse)
library(rowr)
```


```{r make and move report}
make_md <- function(input_name, output_name) {
  # Moves the knitted md and supporting images from the notebooks dir to reports
  # dir.
  # 
  # Args:
  #   input_name: The name of the knitted md. Typically same as the Rmd title.
  #   output_name: A new name for the md in the reports dir.
  #
  # Output: A new md in the reports dir. 
  file.rename(from = here('notebooks', input_name), 
              to = here('reports', output_name))
  
  file.copy(from = list.files(pattern = '*files'), 
            to = here('reports'), recursive = TRUE)
}

make_md('read_format_wq_data.md', 'Data Pipeline and Cleaning.md')
```

# Functions

The data file structure is:
```{r, warning=FALSE, eval=FALSE, echo=TRUE}

|
|-- data
|     |
      |-- raw
      |
      |-- processed
      |
      |-- interim
            |
            |-- date_01/
            |
            |-- date_02/
            |
            |-- ......
            |
            |-- date_i
                  |
                  |-- Atmos Pressure/
                  |-- Conductivity/
                  |-- Depth Pressure/
                  |-- MiniDot/
```

Functions are roughly broken down into functions that modify a dataframe and
functions that pipe dataframes. Modularization allowed for easier debugging of
the functions. create_arima_ready_() is a wrapper that cleans a date directory. 
whole_dir() is a wrapper on top of create_arima_ready() which allows for all date
directories to be cleaned. Date directories are more accurately a sampling period,
the date is just when the data was extracted and the logger reset.

We'll create Skip=1 is required because each csv has a header as the first 
row.
```{r create_df}
create_date_df <- function(path) {
  # Recursively reads csv files and binds them to each other column-wise
  #
  # Args:
  #   path(object): A specified directory to read
  #
  # Returns:
  #   combined_df(dataframe): A dataframe with all csv's columns appended
  my_files <- 
    list.files(path,
               pattern = '*.csv',
               full.names = TRUE,
               recursive = TRUE) 
  combined_df <- data.frame()
  
  for (file in my_files){
    temp <- read_csv(file, skip=1)
    cat('Reading file: ', file, '\n')
    combined_df <- cbind.fill(combined_df, temp, fill=NA)
  }
  return(combined_df)
}
```

```{r clean_df}
clean_date_df <- function(df) {
  # Removes duplicate columns from dataframe, shortens names and changes factors 
  # to numeric when appropriate
  #
  # Args:
  #   df(dataframe): A dataframe with duplicate indexes and times
  #
  # Returns:
  #   df(dataframe): A cleaned dataframe ready for feature engineering
  df <- df[c(3:5, 8, 9, 12, 13)]
  names(df) <- c('date_time', 'surface_pressure', 'air_temp1', 'salinity', 
                 'sal_temp2', 'depth_pressure', 'depth_temp')
  df <- drop_na(df)
  
  # Converts factors to doubles
  factors_to_convert <- sapply(df[,2:7], is.factor)
  df[,2:7][factors_to_convert] <- lapply(df[2:7][factors_to_convert], 
                                         function(x) as.numeric(as.character(x)))
  return(df)
}
```

Our pressure data is in mmHg, but we can convert pressure to meters of water.
```{r create_level}
create_level <- function(df) {
  # Creates the water level in meters, converting the difference of air pressure
  # and depth pressure
  #
  # Args:
  #   df(dataframe): A dataframe with duplicate indexes and times
  #
  # Returns:
  #   df(dataframe): A dataframe ready for a ARIMA model
  conv_factor = 0.013595100263597
  mutate(df, level_m = conv_factor*(df[,'depth_pressure'] - df[,'surface_pressure']))
}
```

```{r df_pipe}
create_arima_ready <- function(path) {
  # A helper function that wraps the creation and cleaning of a single dataframe.
  # Aggregates all the csvs for a single sampling period.
  #
  # Args:
  #   path(object): A specified directory to read
  #
  # Returns:
  #   ready_df(dataframe): A dataframe ready for a ARIMA model
  ready_df <- path %>%
    create_date_df() %>%
    clean_date_df() %>%
    create_level()
  
  return(ready_df)
}
```

```{r dir_wrapper}
whole_dir <- function(path) {
  # A wrapper that list all the sampling period directories and pipes them into
  # create_arima_ready().
  #
  # Args:
  #   path(object): A path to the processed data dir
  #
  # Returns:
  #   arima_ready_dir(list): A list of dataframes ready for a ARIMA 
  #                             model
  csv <- list.files(path, full.names = TRUE)
  arima_ready_dir <- lapply(csv, create_arima_ready)
  return(arima_ready_dir)
}
```

```{r create_csv}
create_csv <- function(list_df, output_name) {
  # Combines dataframes stored in a list into a single dataframe, removes
  # extreme values and creates a output csv. 
  #
  # Args:
  #   list_df(list): A list of dataframes
  #   output_name(string): The name of the output that must end in .csv
  #
  # Returns: Writes a csv to a output directory
  combined_df <- bind_rows(list_df, .id = 'sample_period')
  combined_df <- filter(combined_df, level_m > 1.5)
  
  write_csv(combined_df, here('data', 'processed', output_name))  
}
```

The nice part about wrapping functions is that we only need to define 1 initial 
vairable and call 3 functions. We could have combined whole_dir and create_csv 
into 1 function, however we may want to do data analysis on the seperate 
dataframes.
```{r, message=FALSE, warning=FALSE, results='hide'}
dir_path <- here('data', 'interim')

interim_df <- whole_dir(dir_path)
create_csv(interim_df, 'complete.csv')
```



