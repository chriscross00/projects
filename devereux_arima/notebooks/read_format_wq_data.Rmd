---
title: "read_format_wq_data"
author: "Christoper Chan"
date: "`r format(Sys.time(), '%H:%M %d %B %Y')`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

to do:
- Create a function that outputs a .csv
- Create a function that combines all the csv and outputs a single csv

# Introduction

This notebook is a part 0 of the Devereux Slough time series project. We wrote 
functions that aggregate and cleaning the data as well as some light feature
engineering.

We've chosen to use the library here because it's simplicity and consistency.
Because we have seperate directories for notebooks and data we needed a way to 
navigate between them. While base R does offer this functionality, pathing is 
relative to the current working directory which changes based on where I created
a document. here() offers absolute pathing by setting up a root directory.
```{r libraries, warning=FALSE, message=FALSE}
library(here)
library(tidyverse)
library(rowr)
```


```{r make and move report}
make_md <- function(input_name, output_name) {
  # Moves the knitted md and supporting images from the notebooks dir to reports
  # dir.
  # 
  # Args:
  #   input_name: The name of the knitted md. Typically same as the Rmd title.
  #   output_name: A new name for the md in the reports dir.
  #
  # Output: A new md in the reports dir. 
  file.rename(from = here('notebooks', input_name), 
              to = here('reports', output_name))
  
  file.copy(from = list.files(pattern = '*files'), 
            to = here('reports'), recursive = TRUE)
}

make_md('read_format_wq_data.md', 'read_format_wq_data.md')
```


# Functions


The data file structure is:
```{r, warning=FALSE, eval=FALSE, echo=TRUE}

|
|--- data
|     |
      |--- raw
      |
      |--- interim
      |
      |--- processed
            |
            |--- date_01/
            |
            |--- date_02/
            |
            |--- date_i
                  |
                  |--- Atmos Pressure/
                  |--- Conductivity/
                  |--- Depth Pressure/
                  |--- MiniDot/
```

Functions are roughly broken down into functions that modify a dataframe and
functions that pipe dataframes. Modularization allowed for easier debugging of
the functions. create_arima_ready_() is a wrapper that cleans a date directory. 
whole_dir() is a wrapper on top of create_arima_ready() which allows for all date
directories to be cleaned. Date directories are more accurately a sampling period,
the date is just when the data was extracted and the logger reset.


We'll create Skip=1 is required because each csv has a Plot_title as the first 
row.
```{r create_df}
create_date_df <- function(path) {
  # Recursively reads csv files and binds them to each other column-wise
  #
  # Args:
  #   path(object): A specified directory to read
  #
  # Returns:
  #   combined_df(dataframe): A dataframe with all csv's columns appended
  my_files <- 
    list.files(path,
               pattern = '*.csv',
               full.names = TRUE,
               recursive = TRUE) 
  combined_df <- data.frame()
  
  for (file in my_files){
    temp <- read_csv(file, skip=1)
    cat('Reading file: ', file, '\n')
    combined_df <- cbind.fill(combined_df, temp, fill=NA)
  }
  return(combined_df)
}
```

```{r clean_df}
clean_date_df <- function(df) {
  # Removes duplicate columns from dataframe, shortens names and changes factors 
  # to numeric when appropriate
  #
  # Args:
  #   df(dataframe): A dataframe with duplicate indexes and times
  #
  # Returns:
  #   df(dataframe): A cleaned dataframe ready for feature engineering
  df <- df[c(2:5, 8, 9, 12, 13)]
  names(df) <- c('obs', 'date_time', 'surface_pressure', 'air_temp1', 'salinity', 
                 'sal_temp2', 'depth_pressure', 'depth_temp')
  df <- drop_na(df)
  
  # Converts factors to doubles
  factors_to_convert <- sapply(df[,3:8], is.factor)
  df[,3:8][factors_to_convert] <- lapply(df[3:8][factors_to_convert], 
                                         function(x) as.numeric(as.character(x)))

  return(df)
}
```

Our pressure data is in mmHg, but we can convert pressure to meters of water.
```{r create_level}
create_level <- function(df) {
  # Creates the water level in meters, converting the difference of air pressure
  # and depth pressure
  #
  # Args:
  #   df(dataframe): A dataframe with duplicate indexes and times
  #
  # Returns:
  #   df(dataframe): A dataframe ready for a ARIMA model
  conv_factor = 0.013595100263597
  mutate(df, level_m = conv_factor*(df[,'depth_pressure'] - df[,'surface_pressure']))
}
```

```{r df_pipe}
create_arima_ready <- function(path) {
  # A helper function that wraps the creation and cleaning of a single dataframe.
  # Aggregates all the csvs for a single sampling period.
  #
  # Args:
  #   path(object): A specified directory to read
  #
  # Returns:
  #   ready_df(dataframe): A dataframe ready for a ARIMA model
  ready_df <- path %>%
    create_date_df() %>%
    clean_date_df() %>%
    create_level()
  
  return(ready_df)
}
```

```{r dir_wrapper}
whole_dir <- function(path) {
  # A wrapper that list all the sampling period directories and pipes them into
  # create_arima_ready().
  #
  # Args:
  #   path(object): A path to the processed data dir
  #
  # Returns:
  # arima_ready_dir(dataframe): A collection of dataframes ready for a ARIMA 
  #                             model
  csv <- list.files(path, full.names = TRUE)
  arima_ready_dir <- lapply(csv, create_arima_ready)
  return(arima_ready_dir)
}
```

Test: single dir
```{r}
# test_path <- here('data', 'interim', '180212 Logger Data')
# test1 <- create_arima_ready(test_path)
# 
# str(test1)
# head(test1)
```
Test: all dir

Setting the working directory as data/. This gives me the flexibility to work 
with csvs at different stages of cleaning pipeline.

```{r, message=FALSE, warning=FALSE, results='hide'}
test_dir <- here('data', 'processed')

test5 <- whole_dir(test_dir)
```

