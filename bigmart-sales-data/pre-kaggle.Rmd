---
title: "big_mart"
author: "Christoper Chan"
date: "January 26, 2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(glmnet)
library(randomForest)
library(scales)
```

## 1 Read and clean data

Reading in the data. Reading in the data with read.csv set the categorical variables correctly as factors, while read_csv set them to char.
```{r}
train <- read.csv('Train.csv')
test <- read.csv('Test.csv')

head(train)
```

Creating a combined df so that I can clean/eda on all the available data.
```{r}
test <- mutate(test, Item_Outlet_Sales = 0)


data <- rbind(train, test)

str(data)
```

A quick glance at the data to get a general sense of it, what the data looks like and what I need to clean.
```{r}
summary(data)
```


## 2. Feature Engineering/Cleaning
I want to see if we have redundant levels for categorical variables. In this case we do, in Item_Fat_Content we have 3 types of low fat and 2 types of regular. I'll standardize it to 'Low Fat' and 'Regular'
```{r}
sapply(data[,2:ncol(data)], levels)
```

Finding missing values. Item weight has 2439 NA. Outlet_Size has 4106 blank instances. Every other variable does not have any missing values. In the grand scheme of things I don't think these variables will have much predictive power so I won't bother to fill them in.
```{r}
sapply(data, function(x) sum(is.na(x)))
sapply(data, function(x) sum(x == ''))
```


Now we have to address the NA in Item_Weight. Looking at the distribution of the NA among Item_Types there seems to be some Item_Types that have a larger amount of NA. Note: these values are not normalized so very little inferences can be drawn from this histogram. Instead I'll take the mean of each Item_Type and replace the NA for that type with the mean.
```{r}
item_type_na <- data %>%
    group_by(Item_Type) %>%
    filter(is.na(Item_Weight)) %>%
    count()

ggplot(item_type_na, aes(Item_Type, n)) +
    geom_bar(stat = 'identity') +
    theme(axis.text.x = element_text(angle=75, vjust=0.7))
```

Oh my, so after hours of trying to figure out how to replace all the NAs with the means for that group I came across the answer. 
```{r}
impute_mean <- function(x) replace(x, is.na(x), mean(x, na.rm = T))

data <- data %>%
    group_by(Item_Type) %>%
    mutate(Item_Weight = impute_mean(Item_Weight))
```


Fixing some inconsistencies with the factors for Item_Fat_Content.
```{r, message=FALSE}
data <- data %>%
    mutate(Item_Fat_Content = replace(Item_Fat_Content, Item_Fat_Content == 'LF', 'Low Fat')) %>%
    mutate(Item_Fat_Content = replace(Item_Fat_Content, Item_Fat_Content == 'low fat', 'Low Fat')) %>%
    mutate(Item_Fat_Content = replace(Item_Fat_Content, Item_Fat_Content == 'reg', 'Regular')) %>%
    droplevels()
```


```{r}
ggplot(data, aes(Item_Visibility, Item_Outlet_Sales)) + 
    geom_point(size = 0.75)
```


Having an item with zero Item_Visibility that has sales makes little sense. I'm going to make the assumption that people aren't buying items that are stored in the back of the store. Running with Item_Outlet_Sales == 0 yields 0 results, so all the cases of Item_Visibility are beening bought. We'll set the Item_Visibilty for equal to the mean of Item_Visibility.

```{r}
data <- data %>%
    group_by(Item_Type) %>%
    mutate(Item_Visibility = replace(Item_Visibility, Item_Visibility == 0, mean(Item_Visibility))) %>%
    mutate(Item_Visibility = as.double(Item_Visibility))

summary(data)
```

Changing year to years since started.
```{r}
data <- data %>%
    mutate(Years_Open = 2013 -Outlet_Establishment_Year)
data <- data[, c(1,2,3,4,5,6,7,13,9,10,11,12)]
```

Figuring out what to do with the missing values for the Outlet_Size is tricky. It represents a pretty large chunk of the instances. Breaking the data down into levels for each predictor gives us a much clear picture of what's going on. You can use Baye's theorem here, but intuition will be enough here.
```{r}
data %>%
    group_by(Outlet_Location_Type, Outlet_Type, Outlet_Size) %>%
    count()
```

A visualization of Outlet_Type and Outlet_Size. It looks like blank Outlet_Size that are Outlet_Location_Type = Tier 2 are going to be small, with further evidence from that both are Supermarket Type1.
```{r}
ggplot(data, aes(Outlet_Size, Outlet_Location_Type, color=Outlet_Type)) +
    geom_point()
```

Fixing the blank Outlet_Size. Output verifies that it worked, we no longer have any blank values.
```{r}
data <- data %>%
    mutate(Outlet_Size = replace(Outlet_Size, Outlet_Size == '' & Outlet_Location_Type == 'Tier 2', 'Small')) %>%
    mutate(Outlet_Size = replace(Outlet_Size, Outlet_Size == '' & Outlet_Location_Type == 'Tier 3', 'Medium'))

sapply(data, function(x) sum(x == ''))
```

Possible to condense the Item_Type down into 3 categories; FD-Food, NC-Non-Consumable, DR:Drink. However I'll hold off on this and see how well the model does. I'm not quite sure how necessary this is.
```{r}
data <- data %>%
    separate(Item_Identifier, c('Item_Category', 'Item_Identifier'), sep=2)
data$Item_Category = as.factor(data$Item_Category)
```


## 3. EDA
```{r}
sapply(data, function(x) n_distinct(x))
summary(data)
```


```{r}
ggplot(data, aes(Item_Type, Item_Outlet_Sales)) +
    geom_bar(stat='identity') +
    theme(axis.text.x = element_text(angle=60, vjust=0.75))
```

Are certain Item_Type given more visibilty than others? No clear connection
```{r}
ggplot(data, aes(Item_Type, Item_Visibility)) +
    geom_boxplot() +
    theme(axis.text.x = element_text(angle=90, vjust=0.8))
```

```{r}
ggplot(train, aes(Item_Visibility, Item_Outlet_Sales, color = Item_Type)) + 
    geom_point(size = 0.75)
```


## 4. Model
```{r}
mse <- function(x, y) {
    mean((x - y)^2)
}
```


Creating the train/test split again
```{r}
set.seed(1)
train <- data[1:8523,]
train_reg <- train

train_samp <- sample(1:nrow(train), nrow(train)*0.7)


test <- train[-train_samp,]
test <- test[,-c(2)]

train <- train[train_samp,]
train <- train[,-c(2)]

real_test <- data[8524:nrow(data),]
```

Baseline model
```{r}
mean_sales <- mean(train$Item_Outlet_Sales)

base1 <- tibble(test$Outlet_Identifier, 'Item_Outlet_Sales' = mean_sales)

mse_base1 <- mse(base1[,2], test[,12])
cat('MSE of baseline model:', mse_base1)
```


Baseline linear model
```{r}
store_lm <- lm(Item_Outlet_Sales~., train)

summary(store_lm)

plot(store_lm)
```

The linear model preforms far worse than the baseline model, meaning that it's identifying the incorrect parameters.
```{r}
lm_pred <- predict(store_lm, newx=test)

mse_lm <- mse(lm_pred, test[,12])
cat('Linear model MSE: ', mse_lm)
```


Things I learned:
lm to some extent fitlers out highly correlated predictors. If I ran years_open and Outlet_Estblishment_year at the same time, Years_Open would become NA because signularities.



```{r}
store_rf <- randomForest(Item_Outlet_Sales~., train, mtry=3, importance=T)
store_rf
```

```{r}
pred_rf <- predict(store_rf, newdata=test)
mse(pred_rf, test[,12])
importance(store_rf)
```

```{r}
which.min(store_rf$mse)

sqrt(store_rf$mse[which.min(store_rf$mse)])
```



Tuning RF for the mtry parameter. Results are that 3 is the best and the process stops at mtry=6, meaning the more complex trees preform worse than less complex trees.
```{r}
x1 <- train[,-12]

m1 <- tuneRF(
    x = x1,
    y = train$Item_Outlet_Sales,
    ntreeTry = 250,
    mtryStart = 4,
    stepFactor = 1.5,
    improve = 0.01
)
```


```{r}
feat_imp <- store_rf %>%
    importance(type=1) %>%
    as.tibble() %>%
    rename(Inc_MSE= '%IncMSE') %>%
    mutate(Feature = rownames(importance(store_rf))) %>%
    select(Feature, Inc_MSE) %>%
    mutate(relative_imp = Inc_MSE/sum(Inc_MSE)) %>%
    mutate(Feature = factor(Feature, levels=Feature[order(Inc_MSE)]))


ggplot(feat_imp, aes(Feature, relative_imp)) +
    geom_bar(stat='identity', fill='#56B4E9') +
    coord_flip() +
    ylab('Relative importance') +
    ggtitle('Relative importance of features from Random Forest model')
```

```{r}
total_sales <- data %>%
    group_by(Outlet_Identifier) %>%
    summarise(Total_Sales = sum(Item_Outlet_Sales))

ggplot(total_sales, aes(Outlet_Identifier, Total_Sales)) +
    geom_bar(stat='identity', fill='#56B4E9') +
    ggtitle('Total sales per outlet') +
    ylab('Total sales ($)') +
    xlab('Outlet identifier') +
    geom_text(aes(label=ifelse(Total_Sales==max(Total_Sales), as.integer(Total_Sales), '')), hjust=0.5, vjust=-0.25) +
    geom_text(aes(label=ifelse(Total_Sales==min(Total_Sales), as.integer(Total_Sales), '')), hjust=0.5, vjust=-0.25) +
    scale_y_continuous(labels=dollar, limits=c(0, 4000000)) +
    theme(panel.grid.major = element_line(color = 'gray85'))
```

Item_Fat_Content, no difference
```{r}
ggplot(data, aes(Item_Fat_Content, Item_Outlet_Sales)) +
    geom_boxplot()
```


```{r}
total <- train_reg %>% group_by(Outlet_Type) %>%
    summarise(total_sales = sum(Item_Outlet_Sales))

avg <- train_reg %>% group_by(Outlet_Type) %>%
    summarise(avg_sales = mean(Item_Outlet_Sales))

num <- train_reg %>% group_by(Outlet_Type) %>%
    tally()

store_normalizer <- c(2,6,1,1)
outlet_sales <- merge(total, avg, by='Outlet_Type') %>%
    merge(num, by='Outlet_Type') %>%
    mutate(avg_sales = avg_sales/store_normalizer) %>%
    mutate(n = n/store_normalizer)
outlet_sales
```



Best stores (sum sales for each outlet id)



To do:

Rerun with important features.
Graph features against each other:
Years vs outlet
Types of stores



