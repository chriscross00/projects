---
title: "BigMart Sales Regression"
author: "Christoper Chan"
date: "February 4, 2019"
output: 
    html_document:
        code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(glmnet)
library(randomForest)
```

BigMart is a fictional chain of grocery stores that sells a variety of products, from household supplies to alcohol. Like many other fields BigMart wants to predict the sale of a product. The matter is more pressing because many products have a short shelf life, meaning that the company can potential lose thousands of dollars if they buy the wrong produt. Therefore, our goal is to use data about the products and about each store to predict the future sale of a product.




We'll go through a 5 step process:
- Processing the data
- Feature engineering
- Exploratory data analysis
- Model building
- Analysis


## 1. Read and clean data

Because the dataset is relatively small we can read it all into the R. We combined both the training and test dataset to ensure we got a complete picture of the data.
```{r}
train <- read.csv('Train.csv')
test <- read.csv('Test.csv')

head(train)

test <- mutate(test, Item_Outlet_Sales = 0)

data <- rbind(train, test)
```

The summary function gives us a quick look at the type of data we are working with and where we need to clean.
```{r}
summary(data)
```




Items that are less than 2,500 represent []% of the total sales.
Strategies going forward:

Items:
Item_Fat_Content has __ effect
Unsuprisingly, one of the biggest factors for a person to buy a product is price, MRP. 
sell more of X

Stores:
Stores X and Y need more work
Stores that are X years old tend to do better
X types of stores work better

The reasons why certain stores do better than others is beyond the scope of this dataset, we simply do not have the data to investigate. That being said, finding the reasons why certain stores preform better than others would be a strong first step in increasing sales across all stores. 


## 2. Feature Engineering/Cleaning

Almost all of the features had to be engineered or cleaned in some capacity. Each chunk of code is a seperate feature being altered, this breaks the feature engineering up and keeps the code modular. Features are cleaned in order, from left to right.
```{r item}
data <- data %>%
    separate(Item_Identifier, c('Item_Category', 'Item_Identifier'), sep=2)
data$Item_Category = as.factor(data$Item_Category)
```

```{r}
impute_mean <- function(x) replace(x, is.na(x), mean(x, na.rm = T))

data <- data %>%
    group_by(Item_Type) %>%
    mutate(Item_Weight = impute_mean(Item_Weight))
```

```{r fat, message=FALSE}
data <- data %>%
    mutate(Item_Fat_Content = replace(Item_Fat_Content, Item_Fat_Content == 'LF', 'Low Fat')) %>%
    mutate(Item_Fat_Content = replace(Item_Fat_Content, Item_Fat_Content == 'low fat', 'Low Fat')) %>%
    mutate(Item_Fat_Content = replace(Item_Fat_Content, Item_Fat_Content == 'reg', 'Regular')) %>%
    droplevels()
```

```{r vis}
data <- data %>%
    group_by(Item_Type) %>%
    mutate(Item_Visibility = replace(Item_Visibility, Item_Visibility == 0, mean(Item_Visibility))) %>%
    mutate(Item_Visibility = as.double(Item_Visibility))
```

```{r year}
data <- data %>%
    mutate(Years_Open = 2013 -Outlet_Establishment_Year)
data <- data[, c(1,2,3,4,5,6,7,14,8,10,11,12,13)]
```

```{r outlet}
data <- data %>%
    mutate(Outlet_Size = replace(Outlet_Size, Outlet_Size == '' & Outlet_Location_Type == 'Tier 2', 'Small')) %>%
    mutate(Outlet_Size = replace(Outlet_Size, Outlet_Size == '' & Outlet_Location_Type == 'Tier 3', 'Medium'))
```

Some highlights of the data processing:

- Items with missing weights were replaced with their respective Item_Type's mean.
- 

## 3. EDA
```{r}
sapply(data, function(x) n_distinct(x))
summary(data)
```

Because our response variable is not normally distributed, it is skewed highly right skewed, a key assumption of linear regression is violated, that the response variable is normally distributed. In the context of a grocery store a highly right skewed Item_Outlet_Sales distribution makes sense: people aren't all buying one product, instead a variety of products are being sold at relatively low quantities.

Instead of a linear model we'll run a random forest model which doesn't carry all the assumptions of linear regression.

```{r}
ggplot(data, aes(Item_Outlet_Sales)) + 
    geom_density(fill='#56B4E9') +
    ggtitle('Distribution of the sales')
```


## 4. Model building
```{r mse, echo=FALSE, results='hide'}

mse <- function(x, y) {
    mean((x - y)^2)
}
```


```{r, echo=FALSE}
set.seed(1)
train <- data[1:8523,]
train_reg <- train

train_samp <- sample(1:nrow(train), nrow(train)*0.7)


test <- train[-train_samp,]
test <- test[,-c(2)]

train <- train[train_samp,]
train <- train[,-c(2)]

real_test <- data[8524:nrow(data),]
```


```{r}
summary(test)
```

We constructed a baseline model to see if the more advanced models we create are worth the effort. The baseline model takes the mean of the Item_Outlet_Sales as the predicted sales of each item.
```{r}
mean_sales <- mean(train$Item_Outlet_Sales)

base1 <- tibble(test$Outlet_Identifier, 'Item_Outlet_Sales' = mean_sales)

mse_base1 <- mse(base1[,2], test[,12])
cat('MSE of baseline model:', mse_base1)
```


```{r}
store_rf <- randomForest(Item_Outlet_Sales~., train, mtry=3, importance=T)
store_rf
```

```{r}
plot(store_rf)
```


```{r}
pred_rf <- predict(store_rf, newdata=test)
cat('MSE of the random forest model:', mse(pred_rf, test[,12]))
```



```{r}
feat_imp <- store_rf %>%
    importance(type=1) %>%
    as.tibble() %>%
    rename(Inc_MSE= '%IncMSE') %>%
    mutate(Feature = rownames(importance(store_rf))) %>%
    select(Feature, Inc_MSE) %>%
    mutate(relative_imp = Inc_MSE/sum(Inc_MSE)) %>%
    mutate(Feature = factor(Feature, levels=Feature[order(Inc_MSE)]))


ggplot(feat_imp, aes(Feature, relative_imp)) +
    geom_bar(stat='identity', fill='#56B4E9') +
    coord_flip() +
    ylab('Relative importance') +
    ggtitle('Relative importance of features from Random Forest model')
```



Outlet_Identifier and Item_MRP are highest


Things I learned:
lm to some extent fitlers out highly correlated predictors. If I ran years_open and Outlet_Estblishment_year at the same time, Years_Open would become NA because signularities.
