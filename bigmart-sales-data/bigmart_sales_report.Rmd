---
title: "BigMart Sales Regression"
author: "Christoper Chan"
date: "February 4, 2019"
output: 
    github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(glmnet)
library(randomForest)
library(scales)
```

To see all the code of this project go to: [Full](https://github.com/chriscross00/projects/blob/master/bigmart-sales-data/pre-kaggle.md)



BigMart is a fictional chain of grocery stores that sells a variety of products, from household supplies to alcohol. Like many other fields BigMart wants to predict the sale of a product. The matter is more pressing because many products have a short shelf life, meaning that the company can potential lose thousands of dollars if they buy the wrong produt. Therefore, our goal is to use data about the products and about each store to predict the future sale of a product.

Conclusions:

- Item_MRP (price) is by far the strongest predictor of Item_Outlet_Sales (sales).
- The next best predictors of sales are all related to the outlet. Outlet_Type (with major caveats), Outlet_Identifier, and Years_Open all predict sales reasonably well.
- Fruits and Vegetables and Snack Foods have the highest sales at $5,552,846 and represent 30% of the total sales. Item_Types such as Breakfast and Seafood have the worst total sales and account for only 2% of our total profits.
- The reasons why certain stores do better than others is beyond the scope of this dataset, we simply do not have the data to investigate. 

We'll go through a 5 step process:

- Processing the data
- Feature engineering
- Exploratory data analysis
- Model building
- Analysis


## 1 Read and clean data

Because the dataset is relatively small we can read it all into the R. We combined both the training and test dataset to ensure we got a complete picture of the data.
```{r}
train <- read.csv('Train.csv')
test <- read.csv('Test.csv')

head(train)

test <- mutate(test, Item_Outlet_Sales = 0)

data <- rbind(train, test)
```

The summary function gives us a quick look at the type of data we are working with and where we need to clean.
```{r}
summary(data)
```


## 2 Feature Engineering/Cleaning

Almost all of the features had to be engineered or cleaned in some capacity. Each chunk of code is a seperate feature being altered, this breaks the feature engineering up and keeps the code modular. Features are cleaned in order, from left to right.
```{r item}
data <- data %>%
    separate(Item_Identifier, c('Item_Category', 'Item_Identifier'), sep=2)
data$Item_Category = as.factor(data$Item_Category)
```

```{r weight, message=FALSE, warning=FALSE}
impute_mean <- function(x) replace(x, is.na(x), mean(x, na.rm = T))

data <- data %>%
    group_by(Item_Type) %>%
    mutate(Item_Weight = impute_mean(Item_Weight))
```

```{r fat, message=FALSE}
data <- data %>%
    mutate(Item_Fat_Content = replace(Item_Fat_Content, Item_Fat_Content == 'LF', 'Low Fat')) %>%
    mutate(Item_Fat_Content = replace(Item_Fat_Content, Item_Fat_Content == 'low fat', 'Low Fat')) %>%
    mutate(Item_Fat_Content = replace(Item_Fat_Content, Item_Fat_Content == 'reg', 'Regular')) %>%
    droplevels()
```

```{r vis}
data <- data %>%
    group_by(Item_Type) %>%
    mutate(Item_Visibility = replace(Item_Visibility, Item_Visibility == 0, mean(Item_Visibility))) %>%
    mutate(Item_Visibility = as.double(Item_Visibility))
```

```{r year}
data <- data %>%
    mutate(Years_Open = 2013 -Outlet_Establishment_Year)
data <- data[, c(1,2,3,4,5,6,7,14,8,10,11,12,13)]
```

```{r outlet}
data <- data %>%
    mutate(Outlet_Size = replace(Outlet_Size, Outlet_Size == '' & Outlet_Location_Type == 'Tier 2', 'Small')) %>%
    mutate(Outlet_Size = replace(Outlet_Size, Outlet_Size == '' & Outlet_Location_Type == 'Tier 3', 'Medium'))
```

Some highlights of the data processing:

- Items with missing weights were replaced with their respective Item_Type's mean.
- Item_Fat_Content was standardized.
- Items with zero Item_Visibility made no sense, how could customers buy something they don't know exist in the store. Item_Visibility was set to the mean Item_Visbility of that Item_Type because in theory Item_Types are grouped together in store, ie all the peanut butter is paced next to each other yielding relatively similar Item_Visbility.

## 3 EDA

For the full EDA please see the linked full code. The graphs presented below are the most interesting.
```{r}
sapply(data, function(x) n_distinct(x))
summary(data)
```

Because our response variable is not normally distributed, it is skewed highly right skewed, a key assumption of linear regression is violated, that the response variable is normally distributed. In the context of a grocery store a highly right skewed Item_Outlet_Sales distribution makes sense: people aren't all buying one product, instead a variety of products are being sold at relatively low quantities.

Instead of a linear model we'll run a random forest model which doesn't carry all the assumptions of linear regression.

```{r}
ggplot(data, aes(Item_Outlet_Sales)) + 
    geom_density(fill='#56B4E9') +
    ggtitle('Distribution of the sales')
```


## 4 Model building
```{r mse, echo=FALSE, results='hide'}

mse <- function(x, y) {
    mean((x - y)^2)
}
```


```{r, echo=FALSE}
set.seed(1)
train <- data[1:8523,]
train_reg <- train

train_samp <- sample(1:nrow(train), nrow(train)*0.7)


test <- train[-train_samp,]
test <- test[,-c(2)]

train <- train[train_samp,]
train <- train[,-c(2)]

real_test <- data[8524:nrow(data),]
```


We constructed a baseline model to see if the more advanced models we create are worth the effort. The baseline model takes the mean of the Item_Outlet_Sales as the predicted sales of each item.
```{r}
mean_sales <- mean(train$Item_Outlet_Sales)

base1 <- tibble(test$Outlet_Identifier, 'Item_Outlet_Sales' = mean_sales)

mse_base1 <- mse(base1[,2], test[,12])
cat('MSE of baseline model:', mse_base1)
```

Finally, we ran a random forest model. It preforms far better than the baseline model.
```{r}
store_rf <- randomForest(Item_Outlet_Sales~., train, mtry=3, importance=T)
store_rf

pred_rf <- predict(store_rf, newdata=test)
cat('MSE of the random forest model:', mse(pred_rf, test[,12]))
```

The graph of relative feature importance tells us what features are the best predictors of Item_Outlet_Sales. Item_MRP is by far the greatest predictor of sales while Outlet_Type, Outlet_Identifier, and Years_Open are also good predictors.
```{r}
feat_imp <- store_rf %>%
    importance(type=1) %>%
    as.tibble() %>%
    rename(Inc_MSE= '%IncMSE') %>%
    mutate(Feature = rownames(importance(store_rf))) %>%
    select(Feature, Inc_MSE) %>%
    mutate(relative_imp = Inc_MSE/sum(Inc_MSE)) %>%
    mutate(Feature = factor(Feature, levels=Feature[order(Inc_MSE)]))


ggplot(feat_imp, aes(Feature, relative_imp)) +
    geom_bar(stat='identity', fill='#56B4E9') +
    coord_flip() +
    ylab('Relative importance') +
    ggtitle('Relative importance of features from Random Forest model')
```
## 5 Analysis

Some outlets do much better than other outlets. The maximum sales and minimum sales are labelled below. Most of the outlets have a total sales of around $2,000,000.
```{r}
total_sales <- data %>%
    group_by(Outlet_Identifier, Years_Open) %>%
    summarise(Total_Sales = sum(Item_Outlet_Sales))

ggplot(total_sales, aes(Outlet_Identifier, Total_Sales)) +
    geom_bar(stat='identity', fill='#56B4E9') +
    ggtitle('Total sales per outlet') +
    ylab('Total sales ($)') +
    xlab('Outlet identifier') +
    geom_text(aes(label=ifelse(Total_Sales==max(Total_Sales), as.integer(Total_Sales), '')), hjust=0.5, vjust=-0.25) +
    geom_text(aes(label=ifelse(Total_Sales==min(Total_Sales), as.integer(Total_Sales), '')), hjust=0.5, vjust=-0.25) +
    scale_y_continuous(labels=dollar, limits=c(0, 4000000)) +
    theme(panel.grid.major = element_line(color = 'gray85'))
```

While Outlet_Type is the second strongest predictor of price, the results may be skewed. We don't have a even distribution of Outlet_Types, we have 2 Grocery Stores, 6 Type1, 1 Type2 and 1 Type3 Supermarkets. We don't know if these sales numbers are accurate across the board for each Outlet_Type or if the Grocery Store, Typ2 and Type3 Supermarkets represent a outlier. Therefore, Outlet_Type should be used with caution as a predictor of sales. A much better predictor is Outlet_Identifier, though it lacks some of the generalization power.
```{r}
data %>% 
    group_by(Outlet_Type, Outlet_Identifier) %>%
    tally()
```

```{r}
total <- train_reg %>% group_by(Outlet_Type) %>%
    summarise(total_sales = sum(Item_Outlet_Sales))

avg <- train_reg %>% group_by(Outlet_Type) %>%
    summarise(avg_sales = mean(Item_Outlet_Sales))

num <- train_reg %>% group_by(Outlet_Type) %>%
    tally()

store_normalizer <- c(2,6,1,1)
outlet_sales <- merge(total, avg, by='Outlet_Type') %>%
    merge(num, by='Outlet_Type') %>%
    mutate(avg_sales = avg_sales/store_normalizer) %>%
    mutate(n = n/store_normalizer)
outlet_sales
```

Looking at the items, Fruits and Vegetables and Snack Foods have the highest sales. Together just those two item types bring in $5,552,846 and represent 30% of the total sales across all stores. Item_Types such as Breakfast and Seafood have the worst total sales and account for only 2% of our total profits.

With two types of items, Fruits and Vegetables and Snack Foods, accounting for 30% of the total sales it is essential that we at least maintain those numbers. In particular, because Fruits and Vegetables have a short shelf life, they should be closely watched over time to look for consumer trends so that we do not stock too much at once.
The Breakfast Item_Type has lot's of potential room for improvement. It represents 1.25% of our total sales. Yet, most people eat 3 meals a day. This represents a large part of the market that we are missing. 
```{r}
type <- data %>%
    group_by(Item_Type) %>%
    summarise(Type_Sales = sum(Item_Outlet_Sales)) %>%
    mutate(Item_Type = factor(Item_Type, levels=Item_Type[order(Type_Sales)])) %>%
    mutate(relative_imp = Type_Sales/sum(Type_Sales))

ggplot(type, aes(Item_Type, Type_Sales)) +
    geom_bar(stat='identity', fill='#56B4E9') +
    ggtitle('Total sales per item type') +
    ylab('Total sales ($)') +
    xlab('Item Type') +
    scale_y_continuous(labels=dollar) +
    coord_flip() +
    theme(panel.grid.major = element_line(color = 'gray85'), panel.grid.minor.x = element_line(color = 'gray90'))
```


- Grocery Stores sell far less items and by far have the lowest average sales. They sell far more low priced items than compared to Supermarket Type1.

## Next Steps
The reasons why certain stores do better than others is beyond the scope of this dataset, we simply do not have the data to investigate. That being said, finding the reasons why certain stores preform better than others would be a strong first step in increasing sales across all stores. Once we answer that we can answer the following questions with certainty: 

- Stores X and Y need more work
- Stores that are X years old tend to do better
- X types of stores work better


## Things I learned:

- lm to some extent fitlers out highly correlated predictors. If I ran years_open and Outlet_Estblishment_year at the same time, Years_Open would become NA because signularities.
- Feature engineering is one of the most important steps you can do to increase the accuracy of your model.

